import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin, urlparse
import time
import json

class AdvancedSEOAnalyzer:
    def __init__(self, max_pages=50, delay=1.0):
        self.max_pages = max_pages
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

    def is_valid_url(self, url, domain):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –¥–æ–ø—É—Å—Ç–∏–º—ã–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        parsed = urlparse(url)
        return (
            parsed.scheme in ('http', 'https') and
            parsed.netloc == domain and
            not parsed.fragment and
            not url.endswith(('.pdf', '.jpg', '.png', '.zip'))
        )

    def analyze_images(self, soup, base_url):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        images = []
        for img in soup.find_all('img'):
            img_data = {
                'src': urljoin(base_url, img.get('src', '')),
                'alt': img.get('alt', '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'),
                'width': img.get('width', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                'height': img.get('height', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                'loading': img.get('loading', '–ù–µ —É–∫–∞–∑–∞–Ω–æ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è lazy)')
            }
            images.append(img_data)
        return images

    def analyze_microdata(self, soup):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É (Schema.org, OpenGraph)"""
        microdata = {
            'schema': [],
            'og': {},
            'twitter': {}
        }

        # Schema.org
        for item in soup.find_all(attrs={'itemscope': True}):
            schema_type = item.get('itemtype', '–ù–µ —É–∫–∞–∑–∞–Ω')
            microdata['schema'].append(schema_type)

        # OpenGraph
        for meta in soup.find_all('meta', attrs={'property': True}):
            if meta['property'].startswith('og:'):
                microdata['og'][meta['property']] = meta.get('content', '')

        # Twitter Cards
        for meta in soup.find_all('meta', attrs={'name': True}):
            if meta['name'].startswith('twitter:'):
                microdata['twitter'][meta['name']] = meta.get('content', '')

        return microdata

    def analyze_page(self, url, domain):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ
            title = soup.title.string if soup.title else "‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
            meta_desc = (soup.find('meta', attrs={'name': 'description'})['content'] 
                        if soup.find('meta', attrs={'name': 'description'}) else "‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            headings = {f'h{i}': [h.get_text(strip=True) for h in soup.find_all(f'h{i}')] for i in range(1, 4)}

            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            images = self.analyze_images(soup, url)
            img_errors = sum(1 for img in images if img['alt'] == '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')

            # –ê–Ω–∞–ª–∏–∑ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏
            microdata = self.analyze_microdata(soup)

            return {
                'URL': url,
                'Title': title,
                'Title_Length': len(title),
                'Meta_Description': meta_desc,
                'Meta_Length': len(meta_desc),
                'H1_Count': len(headings['h1']),
                'H2_Count': len(headings['h2']),
                'H3_Count': len(headings['h3']),
                'Images_Total': len(images),
                'Images_Without_Alt': img_errors,
                'Schema_Types': ', '.join(microdata['schema']) if microdata['schema'] else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
                'OG_Tags': len(microdata['og']),
                'Twitter_Tags': len(microdata['twitter']),
                'Status': response.status_code,
                'Domain': domain
            }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {url}: {str(e)}")
            return None

    def crawl_site(self, start_url):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞"""
        domain = urlparse(start_url).netloc
        visited = set()
        results = []
        
        to_visit = {start_url}
        while to_visit and len(visited) < self.max_pages:
            url = to_visit.pop()
            
            if url in visited:
                continue
                
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é: {url}")
            page_data = self.analyze_page(url, domain)
            if page_data:
                results.append(page_data)
            visited.add(url)
            time.sleep(self.delay)
            
            try:
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    full_url = urljoin(url, href)
                    
                    if (self.is_valid_url(full_url, domain) 
                        and full_url not in visited 
                        and len(visited) < self.max_pages):
                        to_visit.add(full_url)
                        
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ {url}: {str(e)}")
        
        return results

def main():
    print("=== –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π SEO-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ===")
    print("–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: site1.ru, site2.com)")
    user_input = input("URL —Å–∞–π—Ç–æ–≤: ").strip()
    
    urls = [url.strip() for url in user_input.split(',') if url.strip()]
    
    analyzer = AdvancedSEOAnalyzer(max_pages=50, delay=1.0)
    all_results = []
    
    for url in urls:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        print(f"\nüîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞: {url}")
        site_results = analyzer.crawl_site(url)
        all_results.extend(site_results)
    
    if all_results:
        df = pd.DataFrame(all_results)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        df['Title_Recommendation'] = df['Title_Length'].apply(
            lambda x: "‚úÖ OK" if 50 <= x <= 60 else "‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π" if x < 50 else "‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        df['Meta_Recommendation'] = df['Meta_Length'].apply(
            lambda x: "‚úÖ OK" if 120 <= x <= 160 else "‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ" if x < 120 else "‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ")
        
        df['Images_Recommendation'] = df.apply(
            lambda x: "‚úÖ OK" if x['Images_Without_Alt'] == 0 
            else f"‚ö†Ô∏è {x['Images_Without_Alt']} –±–µ–∑ alt", axis=1)
        
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M')
        filename = f"advanced_seo_report_{timestamp}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"\nüìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
        print("\n–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∞–π—Ç–∞–º:")
        print(df.groupby('Domain').agg({
            'H1_Count': 'mean',
            'Images_Without_Alt': 'sum',
            'OG_Tags': 'mean',
            'Schema_Types': lambda x: (x != '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç').mean()
        }).round(2))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏
        sample_microdata = {}
        for _, row in df[df['OG_Tags'] > 0].head(2).iterrows():
            sample_microdata[row['URL']] = {
                'OG': json.loads(row.to_json())['OG_Tags'],
                'Schema': row['Schema_Types']
            }
        
        with open(f"microdata_samples_{timestamp}.json", 'w') as f:
            json.dump(sample_microdata, f, indent=2)
        print(f"\n–ü—Ä–∏–º–µ—Ä—ã –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ microdata_samples_{timestamp}.json")
        
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

if __name__ == "__main__":
    main()
